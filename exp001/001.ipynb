{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from rich import print\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import wandb\n",
    "from wandb.lightgbm import wandb_callback, log_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    exp = \"001\"\n",
    "    ver = \"001\"\n",
    "\n",
    "    project_name = f\"homecredit-{exp}-{ver}\"\n",
    "    model_dir = Path(f\"/kaggle/input/{project_name}\")\n",
    "\n",
    "    train_dir = Path(\n",
    "        \"/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/train\"\n",
    "    )\n",
    "    test_dir = Path(\n",
    "        \"/kaggle/input/home-credit-credit-risk-model-stability/parquet_files/test\"\n",
    "    )\n",
    "\n",
    "    file_names = [\n",
    "        \"static_cb_0\",\n",
    "        \"static_0\",\n",
    "        \"debitcard_1\",\n",
    "        \"other_1\",\n",
    "        \"applprev_1\",\n",
    "        \"person_1\",\n",
    "        \"deposit_1\",\n",
    "        \"credit_bureau_a_1\",\n",
    "        \"credit_bureau_b_1\",\n",
    "        \"tax_registry_a_1\",\n",
    "        \"tax_registry_b_1\",\n",
    "        \"tax_registry_c_1\",\n",
    "        \"applprev_2\",\n",
    "        \"person_2\",\n",
    "        \"credit_bureau_a_2\",\n",
    "        \"credit_bureau_b_2\",\n",
    "    ]\n",
    "\n",
    "    is_debug = True\n",
    "    debug_size = 1000\n",
    "\n",
    "    params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"max_depth\": 10,  \n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 2000,  \n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"verbose\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 10,\n",
    "    \"extra_trees\":True,\n",
    "    'num_leaves':64,\n",
    "    \"device\": \"cpu\", \n",
    "    \"is_training_metric\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    def num_expr(df: pl.DataFrame):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        return expr_max + expr_min + expr_last + expr_mean\n",
    "\n",
    "    def date_expr(df: pl.DataFrame):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        return expr_max + expr_min + expr_last + expr_mean\n",
    "\n",
    "    def str_expr(df: pl.DataFrame):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        # expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    def other_expr(df: pl.DataFrame):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    def count_expr(df: pl.DataFrame):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    def all_exprs(df: pl.DataFrame) -> list[pl.Expr]:\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "        return exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataStore = dict[str, list[pl.DataFrame]]\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    def set_table_dtypes(df: pl.DataFrame) -> pl.DataFrame:\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).str.strptime(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Utf8))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).str.strptime(pl.Date))\n",
    "        return df\n",
    "\n",
    "    def handle_dates(df: pl.DataFrame):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n",
    "                # df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n",
    "                df = df.with_columns(pl.col(col).dt.days())  # t - t-1\n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "        return df\n",
    "\n",
    "    def filter_cols(df: pl.DataFrame):\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "                if isnull > 0.7:\n",
    "                    df = df.drop(col)\n",
    "\n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (\n",
    "                df[col].dtype == pl.Utf8\n",
    "            ):\n",
    "                freq = df[col].n_unique()\n",
    "                if (freq == 1) | (freq > 200):\n",
    "                    df = df.drop(col)\n",
    "        return df\n",
    "\n",
    "\n",
    "def feature_eng_base(df_base: pl.DataFrame):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "    return df_base\n",
    "\n",
    "\n",
    "def read_file(file_name: str, depth: int | None = None, is_train: bool = True):\n",
    "    dir = Config.train_dir if is_train else Config.test_dir\n",
    "    n_rows = Config.debug_size if Config.is_debug else None\n",
    "    chunks = []\n",
    "    for path in dir.glob(\"*_\" + file_name + \"*.parquet\"):\n",
    "        df = pl.read_parquet(path, n_rows=n_rows)\n",
    "        df: pl.DataFrame = df.pipe(Pipeline.set_table_dtypes)\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.all_exprs(df))\n",
    "        chunks.append(df)\n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocessing(is_train: bool = True):\n",
    "    df_base = read_file(\"base\")\n",
    "    df_base = feature_eng_base(df_base)\n",
    "\n",
    "    for i, file_name in enumerate(Config.file_names):\n",
    "        depth = int(file_name[-1])\n",
    "        df = read_file(file_name, depth)\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "    df_base: pl.DataFrame = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pandas(df_data: pl.DataFrame, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    return df_data, cat_cols\n",
    "\n",
    "\n",
    "def gini_stability(base: pd.DataFrame):\n",
    "    w_fallingrate = 88.0\n",
    "    w_resstd = -0.5\n",
    "\n",
    "    gini_in_time = (\n",
    "        base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\n",
    "        .sort_values(\"WEEK_NUM\")\n",
    "        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\n",
    "        .apply(lambda x: 2 * roc_auc_score(x[\"target\"], x[\"score\"]) - 1)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    x = np.arange(len(gini_in_time))\n",
    "    y = gini_in_time\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    y_hat = a * x + b\n",
    "    residuals = y - y_hat\n",
    "    res_std = np.std(residuals)\n",
    "    avg_gini = np.mean(gini_in_time)\n",
    "    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = preprocessing()\n",
    "df, _ = to_pandas(df)\n",
    "kf = StratifiedGroupKFold(n_splits=2)\n",
    "base = df[[\"case_id\", \"WEEK_NUM\", \"target\"]]\n",
    "base[\"score\"] = 0\n",
    "X = df.drop([\"case_id\", \"WEEK_NUM\", \"target\"], axis=1)\n",
    "y = df[\"target\"]\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(X, y, groups=base[\"WEEK_NUM\"])):\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    base_train = base.iloc[train_index]\n",
    "\n",
    "    X_valid = X.iloc[valid_index]\n",
    "    y_valid = y.iloc[valid_index]\n",
    "    base_valid = base.iloc[valid_index]\n",
    "    \n",
    "\n",
    "    train_ds = lgb.Dataset(X_train, y_train)\n",
    "    valid_ds = lgb.Dataset(X_valid, y_valid)\n",
    "\n",
    "    gbm = lgb.train(\n",
    "        Config.params, train_ds, valid_sets=[train_ds, valid_ds],\n",
    "    )\n",
    "\n",
    "    y_pred = gbm.predict(X_valid, num_iteration=gbm.best_iteration)\n",
    "    base.loc[valid_index, \"score\"] = y_pred\n",
    "    print(f\"stability score fold {i}: {gini_stability(base.iloc[valid_index])}\")\n",
    "    \n",
    "    gbm.save_model(Config.model_dir / f\"gbm_{i}.txt\")\n",
    "print(f\"stability score: {gini_stability(base)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
